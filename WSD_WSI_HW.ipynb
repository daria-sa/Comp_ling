{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "WSD_WSI_HW.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sm11bWNIydn2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import adagram\n",
        "from lxml import html\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet as wn\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "from string import punctuation\n",
        "import json, os\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from pathlib import Path\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "punct = punctuation+'«»—…“”*№–'\n",
        "morph = MorphAnalyzer()\n",
        "stops = set(stopwords.words('russian'))\n",
        "from sklearn.decomposition import TruncatedSVD, NMF, PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import gensim\n",
        "import numpy as np\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, cross_validate\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "\n",
        "def tokenize(text):\n",
        "    \n",
        "    words = [word.strip(punct) for word in text.lower().split() if word and word not in stops]\n",
        "    words = [word for word in words if word]\n",
        "\n",
        "    return words\n",
        "\n",
        "def normalize(text):\n",
        "    \n",
        "    words = tokenize(text)\n",
        "    words = [morph.parse(word)[0].normal_form for word in words if word]\n",
        "\n",
        "    return words"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA-nNFeEylT-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "285e952e-710e-4adb-c234-480821ee3784"
      },
      "source": [
        "!pip install Cython numpy\n",
        "!pip install git+https://github.com/lopuhin/python-adagram.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (0.29.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)\n",
            "Collecting git+https://github.com/lopuhin/python-adagram.git\n",
            "  Cloning https://github.com/lopuhin/python-adagram.git to /tmp/pip-req-build-n161j5d9\n",
            "  Running command git clone -q https://github.com/lopuhin/python-adagram.git /tmp/pip-req-build-n161j5d9\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from adagram==0.0.1) (0.29.20)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from adagram==0.0.1) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.9 in /usr/local/lib/python3.6/dist-packages (from adagram==0.0.1) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from adagram==0.0.1) (1.12.0)\n",
            "Building wheels for collected packages: adagram\n",
            "  Building wheel for adagram (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for adagram: filename=adagram-0.0.1-cp36-cp36m-linux_x86_64.whl size=460304 sha256=99b45ae596edb82e4f221fe16cea7a7e7201d3bc32cb6be85d8884fbf091ac7e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-gsyp52t6/wheels/11/0f/46/f5df96670df8f7973b4c2311ffc9b02e435a7bd3207f992c4d\n",
            "Successfully built adagram\n",
            "Installing collected packages: adagram\n",
            "Successfully installed adagram-0.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Afrcz0wyyynE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c05a63d1-7514-4d3d-a25d-2a3c99cc1f81"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKTnWK2vydoC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9262dc1a-4bda-4a37-dc84-08fda2f8c0bf"
      },
      "source": [
        "# модель обученная на большом корпусе (острожно 1.5 гб)\n",
        "!curl 'https://s3.amazonaws.com/kostia.lopuhin/all.a010.p10.d300.w5.m100.nonorm.slim.joblib' --output all.a010.p10.d300.w5.m100.nonorm.slim.joblib\n",
        "vm = adagram.VectorModel.load('all.a010.p10.d300.w5.m100.nonorm.slim.joblib')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1394M  100 1394M    0     0  45.3M      0  0:00:30  0:00:30 --:--:-- 47.4M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDdFjgAFydoG",
        "colab_type": "text"
      },
      "source": [
        "## Задание № 1. Протестировать адаграм в определении перефразирования"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DznVHQWkydoH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0h1Kl-2v4I3-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "63fc4f3d-6186-41bb-f20c-7fad553d9921"
      },
      "source": [
        "%cd /"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDYknJpm3wWt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aae127ad-0cba-4209-f7e6-d7e3dc09302b"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/WSI\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/WSI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oV6TzH6mydoL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_xml = html.fromstring(open('paraphraser/paraphrases.xml', 'rb').read())\n",
        "texts_1 = []\n",
        "texts_2 = []\n",
        "classes = []\n",
        "\n",
        "for p in corpus_xml.xpath('//paraphrase'):\n",
        "    texts_1.append(p.xpath('./value[@name=\"text_1\"]/text()')[0])\n",
        "    texts_2.append(p.xpath('./value[@name=\"text_2\"]/text()')[0])\n",
        "    classes.append(p.xpath('./value[@name=\"class\"]/text()')[0])\n",
        "    \n",
        "data = pd.DataFrame({'text_1':texts_1, 'text_2':texts_2, 'label':classes})"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9qDFxBBydoO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['text_1_norm'] = data['text_1'].apply(normalize)\n",
        "data['text_2_norm'] = data['text_2'].apply(normalize)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRlHXB_TydoR",
        "colab_type": "text"
      },
      "source": [
        "Векторизуйте пары текстов с помощью Адаграма, обучите любую модель и оцените качество (кросс-валидацией). \n",
        "\n",
        "За основу возьмите код из предыдущего семинара/домашки, только в функции get_embedding вам нужно выбирать вектор нужного значения (импользуйте model.disambiguate и model.sense_vector). Отдельные векторы усредните как и в предыдущем семинаре."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHQ9AL_iydoR",
        "colab_type": "text"
      },
      "source": [
        "Для вытаскивания пар (целевое слово, контекстые слова) вам нужно будет написать специальную функцию."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxnLwEnrydoS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# проверяте на списке из чисел, чтобы было удобно дебажить\n",
        "words = [0,1,2,3,4,5,6,7,8,9]\n",
        "\n",
        "def get_words_in_context(words, window=3):\n",
        "    words_in_context = []\n",
        "    for idx, word in enumerate(words):\n",
        "        context = []\n",
        "        for ctx_idx, ctx_word in enumerate(words):\n",
        "            residual = abs(idx - ctx_idx)\n",
        "            if residual > 0 and residual <= window:\n",
        "                context.append(ctx_word)\n",
        "        words_in_context.append([word, context])\n",
        "    return words_in_context"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "PDLKSTh4ydoX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "2df661ce-9b0e-422f-a522-5bf25cadd5bd"
      },
      "source": [
        "get_words_in_context(words)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, [1, 2, 3]],\n",
              " [1, [0, 2, 3, 4]],\n",
              " [2, [0, 1, 3, 4, 5]],\n",
              " [3, [0, 1, 2, 4, 5, 6]],\n",
              " [4, [1, 2, 3, 5, 6, 7]],\n",
              " [5, [2, 3, 4, 6, 7, 8]],\n",
              " [6, [3, 4, 5, 7, 8, 9]],\n",
              " [7, [4, 5, 6, 8, 9]],\n",
              " [8, [5, 6, 7, 9]],\n",
              " [9, [6, 7, 8]]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4QkNJVzydoa",
        "colab_type": "text"
      },
      "source": [
        "Когда получиться такой же результат, добавьте эту функцию в get_embedding. Проходите циклом по результату работы get_words_in_context и поставляйте каждый элемент-список в model.disambiguate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCM9g0B8ydob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_embedding_adagram(text, model, window, dim):\n",
        "    \n",
        "    word2context = get_words_in_context(text, window)\n",
        "    \n",
        "    \n",
        "    vectors = np.zeros((len(word2context), dim))\n",
        "    \n",
        "    for i, (word, context) in enumerate(word2context):\n",
        "        \n",
        "        try:\n",
        "            sense_id = model.disambiguate(word, context).argmax()\n",
        "            vectors[i] = model.sense_vector(word, sense_id)\n",
        "        \n",
        "        except (KeyError, ValueError):\n",
        "            continue\n",
        "    \n",
        "    if vectors.any():\n",
        "        vector = np.average(vectors, axis=0)\n",
        "    else:\n",
        "        vector = np.zeros((dim))\n",
        "    \n",
        "    return vector\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvWye9x8ydoh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dim = vm.dim\n",
        "X_text_1_w2v = np.zeros((len(data['text_1_norm']), dim))\n",
        "X_text_2_w2v = np.zeros((len(data['text_2_norm']), dim))\n",
        "\n",
        "for i, text in enumerate(data['text_1_norm'].values):\n",
        "    X_text_1_w2v[i] = get_embedding_adagram(text, vm, 3, dim)\n",
        "\n",
        "for i, text in enumerate(data['text_2_norm'].values):\n",
        "    X_text_2_w2v[i] = get_embedding_adagram(text, vm, 3, dim)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tz9ZdqOCydol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_text_w2v = np.concatenate([X_text_1_w2v, X_text_2_w2v], axis=1)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "237ID3mJydoo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "db40406b-9870-41e5-a55f-dfde56982fae"
      },
      "source": [
        "y = data['label'].values\n",
        "print(y.shape)\n",
        "train_X, valid_X, train_y, valid_y = train_test_split(X_text_w2v, y,random_state=1)\n",
        "clf = RandomForestClassifier(n_estimators=150, max_depth=6, min_samples_leaf=15,\n",
        "                             class_weight='balanced')\n",
        "clf.fit(train_X, train_y)\n",
        "preds = clf.predict(valid_X)\n",
        "print(classification_report(valid_y, preds))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7227,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.53      0.51      0.52       629\n",
            "           0       0.48      0.50      0.49       737\n",
            "           1       0.38      0.38      0.38       441\n",
            "\n",
            "    accuracy                           0.47      1807\n",
            "   macro avg       0.46      0.46      0.46      1807\n",
            "weighted avg       0.47      0.47      0.47      1807\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzOSoK3dydoz",
        "colab_type": "text"
      },
      "source": [
        "## WordNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrNc4aKaydo0",
        "colab_type": "text"
      },
      "source": [
        "Выводить значения просто из текста тяжело, поэтому можно попробовать воспользоваться словарями (wsi сделанное людьми). Для русского придется парсить сайты словарей, а для английского можно воспользоваться WordNet'ом (https://wordnet.princeton.edu/), который есть в nltk."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrhoUSJNydo1",
        "colab_type": "text"
      },
      "source": [
        "WordNet - это лексическая база данных, где существительные, прилагательные и глаголы английского сгруппированы по значению. К тому же между ними установлены связи (гипонимия, гипоронимия, миронимия и т.п.)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nymTJRe-ydo1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "52c71151-3795-481b-ddbf-4323f287511b"
      },
      "source": [
        "# запустите если не установлен ворднет\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaOVlXLhydo6",
        "colab_type": "text"
      },
      "source": [
        "### Задание 2. Реализовать алгоритм Леска и проверить его на реальном датасете"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5nHOjgEydo6",
        "colab_type": "text"
      },
      "source": [
        "Ворднет можно использовать для дизамбигуации. Самый простой алгоритм дизамбигуации - алгоритм Леска. В нём нужное значение слова находится через пересечение слов контекста, в котором употреблено это слово, с определениями значений слова из ворднета. Значение с максимальным пересечением - нужное."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3Uelww9ydo6",
        "colab_type": "text"
      },
      "source": [
        "Реализуйте его"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTjg734Fydo7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lesk(word, sentence):\n",
        "    \"\"\"Ваш код тут\"\"\"\n",
        "    sentence = set(sentence)\n",
        "    bestsense = 0\n",
        "    maxoverlap = 0\n",
        "    synsets = wn.synsets(word)\n",
        "    for i, syns in enumerate(synsets):\n",
        "        definition = set(syns.definition().lower().split())\n",
        "        overlap = len(sentence & definition)\n",
        "        if overlap > maxoverlap:\n",
        "            maxoverlap = overlap\n",
        "            bestsense = i\n",
        "    return bestsense"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4sD5HMGydo8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ea8cba63-0262-4675-b1d1-a806d303756e"
      },
      "source": [
        "lesk('day', 'some point or period in time'.split())"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4R2_dDiydo-",
        "colab_type": "text"
      },
      "source": [
        "Работать функция должна как-то так:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PeTp2w_ydo-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bb409b3b-2935-4af5-f35d-02873fe9b2e2"
      },
      "source": [
        "# на вход подается элемент результата работы уже написанной вами функции get_words_in_context\n",
        "lesk('day', 'some point or period in time'.split()) # для примера контекст совпадает с одним из определений\n",
        "# а на выходе индекс подходящего синсета"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_BvsyZZydpB",
        "colab_type": "text"
      },
      "source": [
        "**Проверьте насколько хорошо работает такой метод на реальном датасете.** http://lcl.uniroma1.it/wsdeval/ - большой фреймворк для оценки WSD. Там много данных и я взял кусочек, чтобы не было проблем с памятью"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlM0f3eoydpB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_wsd = []\n",
        "corpus = open('corpus_wsd_50k.txt').read().split('\\n\\n')\n",
        "for sent in corpus:\n",
        "    corpus_wsd.append([s.split('\\t') for s in sent.split('\\n')])"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s5Bp9sSydpE",
        "colab_type": "text"
      },
      "source": [
        "Корпус состоит из предложений, где у каждого слова три поля - значение, лемма и само слово. Значение пустое, когда слово однозначное, а у многозначных слов стоит тэг вида **'long%3:00:02::'** Это тэг wordnet'ного формата"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYBPGpAQ-JiO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "8a3bec87-9525-47f7-f4e3-0df11abea7b3"
      },
      "source": [
        "corpus_wsd[0:1]"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[['', 'how', 'How'],\n",
              "  ['long%3:00:02::', 'long', 'long'],\n",
              "  ['', 'have', 'has'],\n",
              "  ['', 'it', 'it'],\n",
              "  ['be%2:42:03::', 'be', 'been'],\n",
              "  ['', 'since', 'since'],\n",
              "  ['', 'you', 'you'],\n",
              "  ['review%2:31:00::', 'review', 'reviewed'],\n",
              "  ['', 'the', 'the'],\n",
              "  ['objective%1:09:00::', 'objective', 'objectives'],\n",
              "  ['', 'of', 'of'],\n",
              "  ['', 'you', 'your'],\n",
              "  ['benefit%1:21:00::', 'benefit', 'benefit'],\n",
              "  ['', 'and', 'and'],\n",
              "  ['service%1:04:07::', 'service', 'service'],\n",
              "  ['program%1:09:01::', 'program', 'program'],\n",
              "  ['', '?', '?']]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYZssbSxydpF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7cbca2df-fd97-48d3-d7bc-d0fc8b2ce4ed"
      },
      "source": [
        "from copy import copy\n",
        "correct_count = 0\n",
        "word_count = 0\n",
        "tokens = []\n",
        "for sent in corpus_wsd:\n",
        "    \n",
        "    for word in sent:\n",
        "      if len(word) == 3:\n",
        "        tokens.append(word[1])\n",
        "      else:\n",
        "        continue\n",
        "      context = copy(sent)\n",
        "      context.remove(word)\n",
        "      context = [word[1] for word in context ]\n",
        "      word_count += 1\n",
        "      if word[0] != '':\n",
        "          real_synset = wn.lemma_from_key(word[0]).synset()\n",
        "          predicted_synset = wn.synsets(word[1])[lesk(word[1], context)]\n",
        "          if predicted_synset == real_synset:\n",
        "              correct_count += 1\n",
        "\n",
        "print(correct_count / word_count)\n",
        "        \n",
        "                    "
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.06498187328413578\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_EubDqgydpG",
        "colab_type": "text"
      },
      "source": [
        "**Вам нужно для каждого многозначного слова (т.е. у него есть тэг в первом поле) с помощью алгоритма Леска предсказать нужный синсет и сравнить с правильным. Посчитайте процент правильных предсказаний (accuracy).**\n",
        "\n",
        "Если считается слишком долго, возьмите поменьше предложений (например, только тысячу)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WSHi-UCydpH",
        "colab_type": "text"
      },
      "source": [
        "### Дополнительный балл"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGc2vCRlydpH",
        "colab_type": "text"
      },
      "source": [
        "Если хотите заработать дополнительный балл, попробуйте улучшить алгоритм Леска любым способом (например, использовать расстояние редактирования вместо пересечения или даже вставить машинное обучение)"
      ]
    }
  ]
}