{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deeppavlov\n",
      "  Using cached deeppavlov-0.7.1-py3-none-any.whl (735 kB)\n",
      "Collecting numpy==1.16.4\n",
      "  Downloading numpy-1.16.4-cp36-cp36m-manylinux1_x86_64.whl (17.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.3 MB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy==1.3.0\n",
      "  Using cached scipy-1.3.0-cp36-cp36m-manylinux1_x86_64.whl (25.2 MB)\n",
      "Collecting fastapi==0.38.1\n",
      "  Using cached fastapi-0.38.1-py3-none-any.whl (160 kB)\n",
      "Collecting fuzzywuzzy==0.17.0\n",
      "  Using cached fuzzywuzzy-0.17.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting overrides==1.9\n",
      "  Using cached overrides-1.9.tar.gz (3.4 kB)\n",
      "Collecting uvicorn==0.9.0\n",
      "  Using cached uvicorn-0.9.0.tar.gz (24 kB)\n",
      "Collecting h5py==2.9.0\n",
      "  Using cached h5py-2.9.0-cp36-cp36m-manylinux1_x86_64.whl (2.8 MB)\n",
      "Collecting keras==2.2.4\n",
      "  Using cached Keras-2.2.4-py2.py3-none-any.whl (312 kB)\n",
      "Requirement already satisfied: requests==2.22.0 in /usr/local/lib/python3.6/dist-packages (from deeppavlov) (2.22.0)\n",
      "Collecting pytelegrambotapi==3.6.6\n",
      "  Using cached pyTelegramBotAPI-3.6.6.tar.gz (49 kB)\n",
      "Collecting Cython==0.29.12\n",
      "  Using cached Cython-0.29.12-cp36-cp36m-manylinux1_x86_64.whl (2.1 MB)\n",
      "Collecting pymorphy2==0.8\n",
      "  Using cached pymorphy2-0.8-py2.py3-none-any.whl (46 kB)\n",
      "Collecting pymorphy2-dicts-ru\n",
      "  Using cached pymorphy2_dicts_ru-2.4.404381.4453942-py2.py3-none-any.whl (8.0 MB)\n",
      "Collecting pyopenssl==19.0.0\n",
      "  Using cached pyOpenSSL-19.0.0-py2.py3-none-any.whl (53 kB)\n",
      "Collecting tqdm==4.32.2\n",
      "  Using cached tqdm-4.32.2-py2.py3-none-any.whl (50 kB)\n",
      "Collecting rusenttokenize==0.0.5\n",
      "  Using cached rusenttokenize-0.0.5-py3-none-any.whl (10 kB)\n",
      "Collecting nltk==3.2.5\n",
      "  Using cached nltk-3.2.5.tar.gz (1.2 MB)\n",
      "Collecting scikit-learn==0.21.2\n",
      "  Using cached scikit_learn-0.21.2-cp36-cp36m-manylinux1_x86_64.whl (6.7 MB)\n",
      "Collecting pandas==0.24.2\n",
      "  Using cached pandas-0.24.2-cp36-cp36m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Collecting starlette<=0.12.8,>=0.11.1\n",
      "  Using cached starlette-0.12.8.tar.gz (45 kB)\n",
      "Collecting pydantic<=0.32.2,>=0.32.2\n",
      "  Using cached pydantic-0.32.2-cp36-cp36m-manylinux1_x86_64.whl (5.1 MB)\n",
      "Collecting click==7.*\n",
      "  Using cached Click-7.0-py2.py3-none-any.whl (81 kB)\n",
      "Collecting h11==0.8.*\n",
      "  Using cached h11-0.8.1-py2.py3-none-any.whl (55 kB)\n",
      "Collecting websockets==8.*\n",
      "  Using cached websockets-8.1-cp36-cp36m-manylinux2010_x86_64.whl (78 kB)\n",
      "Collecting httptools==0.0.13\n",
      "  Using cached httptools-0.0.13.tar.gz (104 kB)\n",
      "Collecting uvloop==0.*\n",
      "  Using cached uvloop-0.14.0-cp36-cp36m-manylinux2010_x86_64.whl (3.9 MB)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py==2.9.0->deeppavlov) (1.14.0)\n",
      "Collecting pyyaml\n",
      "  Using cached PyYAML-5.3.tar.gz (268 kB)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4->deeppavlov) (1.1.0)\n",
      "Collecting keras-applications>=1.0.6\n",
      "  Using cached Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->deeppavlov) (1.25.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->deeppavlov) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.22.0->deeppavlov) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests==2.22.0->deeppavlov) (2.6)\n",
      "Processing ./.cache/pip/wheels/3f/2a/fa/4d7a888e69774d5e6e855d190a8a51b357d77cc05eb1c097c9/docopt-0.6.2-py2.py3-none-any.whl\n",
      "Collecting pymorphy2-dicts<3.0,>=2.4\n",
      "  Using cached pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1 MB)\n",
      "Collecting dawg-python>=0.7\n",
      "  Using cached DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
      "Collecting cryptography>=2.3\n",
      "  Using cached cryptography-2.8-cp34-abi3-manylinux2010_x86_64.whl (2.3 MB)\n",
      "Collecting joblib>=0.11\n",
      "  Using cached joblib-0.14.1-py2.py3-none-any.whl (294 kB)\n",
      "Collecting pytz>=2011k\n",
      "  Using cached pytz-2019.3-py2.py3-none-any.whl (509 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas==0.24.2->deeppavlov) (2.8.1)\n",
      "Collecting dataclasses>=0.6; python_version < \"3.7\"\n",
      "  Using cached dataclasses-0.7-py3-none-any.whl (18 kB)\n",
      "Collecting cffi!=1.11.3,>=1.8\n",
      "  Using cached cffi-1.13.2-cp36-cp36m-manylinux1_x86_64.whl (397 kB)\n",
      "Collecting pycparser\n",
      "  Using cached pycparser-2.19.tar.gz (158 kB)\n",
      "Building wheels for collected packages: overrides, uvicorn, pytelegrambotapi, nltk, starlette, httptools, pyyaml, pycparser\n",
      "  Building wheel for overrides (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for overrides: filename=overrides-1.9-py3-none-any.whl size=5047 sha256=e4d591734025640a7fd1122d8cb5362f52bfcba225e4a07e5d9daec15ec138c4\n",
      "  Stored in directory: /root/.cache/pip/wheels/3a/d7/09/26c2c5c02d09a8ce0501a57dbace9b594a0ca5047602b4963a\n",
      "  Building wheel for uvicorn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for uvicorn: filename=uvicorn-0.9.0-py3-none-any.whl size=38246 sha256=51f56ad1a1c857994ccb20775385ac336d9955e15dfe4d2a1e8b3125da145691\n",
      "  Stored in directory: /root/.cache/pip/wheels/1a/45/f0/103ef686d936adc0a4a48d558d5336a97028fbd6749f252512\n",
      "  Building wheel for pytelegrambotapi (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pytelegrambotapi: filename=pyTelegramBotAPI-3.6.6-py3-none-any.whl size=55071 sha256=00d79d5eeaf07637a3c9c9e0ecbc76d51bd59ad948ed08be59378d5e8efa9ef5\n",
      "  Stored in directory: /root/.cache/pip/wheels/2d/6f/de/9f4137e702efc45886aa54dcef7bd330809f3edabb130fccae\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.2.5-py3-none-any.whl size=1392927 sha256=482bfaa31f2915338e79e788462c62dd3ae93ab58548b7a5ad37b3c120f5fd2e\n",
      "  Stored in directory: /root/.cache/pip/wheels/f2/7f/71/cb36468789a03b5e2908281c8e1ce093e6860258b6b61677d8\n",
      "  Building wheel for starlette (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for starlette: filename=starlette-0.12.8-py3-none-any.whl size=59349 sha256=bfa4481cc42b50834ebbb95192f90daffd80ba3bc633c209de4c9f81ad0db07b\n",
      "  Stored in directory: /root/.cache/pip/wheels/6f/f2/7b/47ed183e00e89152859f12c95caf9951a34056d383bbd8f01c\n",
      "  Building wheel for httptools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for httptools: filename=httptools-0.0.13-cp36-cp36m-linux_x86_64.whl size=213788 sha256=339b5559f538284a8bbe2fc2a52cae238e80e2aab1b23a286bf8095194fc6e6a\n",
      "  Stored in directory: /root/.cache/pip/wheels/10/f6/5e/1b89aee7bd3b1470f15c92054e684439ab17efb3914f63f104\n",
      "  Building wheel for pyyaml (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyyaml: filename=PyYAML-5.3-cp36-cp36m-linux_x86_64.whl size=45519 sha256=66731a2b66e1eec759c303931e9abf5965eb283f752c4325f1d9b731f13f5661\n",
      "  Stored in directory: /root/.cache/pip/wheels/b1/86/0d/10e6c39d3a2b85ba807d7657ee80f08cc16c03f2aa2adf8e46\n",
      "  Building wheel for pycparser (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pycparser: filename=pycparser-2.19-py2.py3-none-any.whl size=112040 sha256=48bde97653b391aa804552a49301140f938eb18165745293e19cae48931c882e\n",
      "  Stored in directory: /root/.cache/pip/wheels/c6/6b/83/2608afaa57ecfb0a66ac89191a8d9bad71c62ca55ee499c2d0\n",
      "Successfully built overrides uvicorn pytelegrambotapi nltk starlette httptools pyyaml pycparser\n",
      "\u001b[31mERROR: Error checking for conflicts.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3021, in _dep_map\n",
      "    return self.__dep_map\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2815, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _DistInfoDistribution__dep_map\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3012, in _parsed_pkg_info\n",
      "    return self._pkg_info\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2815, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _pkg_info\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/commands/install.py\", line 517, in _warn_about_conflicts\n",
      "    package_set, _dep_info = check_install_conflicts(to_install)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/operations/check.py\", line 114, in check_install_conflicts\n",
      "    package_set, _ = create_package_set_from_installed()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/operations/check.py\", line 53, in create_package_set_from_installed\n",
      "    package_set[name] = PackageDetails(dist.version, dist.requires())\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2736, in requires\n",
      "    dm = self._dep_map\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3023, in _dep_map\n",
      "    self.__dep_map = self._compute_dependencies()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3032, in _compute_dependencies\n",
      "    for req in self._parsed_pkg_info.get_all('Requires-Dist') or []:\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3014, in _parsed_pkg_info\n",
      "    metadata = self.get_metadata(self.PKG_INFO)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 1420, in get_metadata\n",
      "    value = self._get(path)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 1616, in _get\n",
      "    with open(path, 'rb') as stream:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/usr/local/lib/python3.6/dist-packages/numpy-1.18.1.dist-info/METADATA'\u001b[0m\n",
      "Installing collected packages: numpy, scipy, starlette, dataclasses, pydantic, fastapi, fuzzywuzzy, overrides, click, h11, websockets, httptools, uvloop, uvicorn, h5py, pyyaml, keras-applications, keras, pytelegrambotapi, Cython, docopt, pymorphy2-dicts, dawg-python, pymorphy2, pymorphy2-dicts-ru, pycparser, cffi, cryptography, pyopenssl, tqdm, rusenttokenize, nltk, joblib, scikit-learn, pytz, pandas, deeppavlov\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.18.1\n",
      "    Can't uninstall 'numpy'. No files were found to uninstall.\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.4.1\n",
      "    Can't uninstall 'scipy'. No files were found to uninstall.\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 2.10.0\n",
      "    Can't uninstall 'h5py'. No files were found to uninstall.\n",
      "  Attempting uninstall: cryptography\n",
      "    Found existing installation: cryptography 2.1.4\n",
      "    Can't uninstall 'cryptography'. No files were found to uninstall.\n",
      "Successfully installed Cython-0.29.12 cffi-1.13.2 click-7.0 cryptography-2.8 dataclasses-0.7 dawg-python-0.7.2 deeppavlov-0.7.1 docopt-0.6.2 fastapi-0.38.1 fuzzywuzzy-0.17.0 h11-0.8.1 h5py-2.9.0 httptools-0.0.13 joblib-0.14.1 keras-2.2.4 keras-applications-1.0.8 nltk-3.2.5 numpy-1.16.4 overrides-1.9 pandas-0.24.2 pycparser-2.19 pydantic-0.32.2 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985 pymorphy2-dicts-ru-2.4.404381.4453942 pyopenssl-19.0.0 pytelegrambotapi-3.6.6 pytz-2019.3 pyyaml-5.3 rusenttokenize-0.0.5 scikit-learn-0.21.2 scipy-1.3.0 starlette-0.12.8 tqdm-4.32.2 uvicorn-0.9.0 uvloop-0.14.0 websockets-8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install deeppavlov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==1.15.0\n",
      "  Downloading tensorflow-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (412.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 412.3 MB 53 kB/s s eta 0:00:01    |████▎                           | 54.5 MB 55.8 MB/s eta 0:00:07\n",
      "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.0.8)\n",
      "Collecting tensorboard<1.16.0,>=1.15.0\n",
      "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 17.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.1.8)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.1.0)\n",
      "Collecting tensorflow-estimator==1.15.1\n",
      "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
      "\u001b[K     |████████████████████████████████| 503 kB 21.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.26.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.14.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (3.11.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.16.4)\n",
      "Collecting astor>=0.6.0\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.9.0)\n",
      "Collecting gast==0.2.2\n",
      "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (3.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorflow==1.15.0) (0.30.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.11.2)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.0) (2.9.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (45.1.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.1.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (0.16.0)\n",
      "Building wheels for collected packages: gast\n",
      "  Building wheel for gast (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7635 sha256=176d2a11b23734bc36b7bc9d2fef4ace0cfeaba042adcb6a7386133a1d02b88a\n",
      "  Stored in directory: /root/.cache/pip/wheels/19/a7/b9/0740c7a3a7d1d348f04823339274b90de25fbcd217b2ee1fbe\n",
      "Successfully built gast\n",
      "\u001b[31mERROR: tf-nightly-gpu 2.2.0.dev20200121 has requirement gast==0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tf-nightly-gpu 2.2.0.dev20200121 has requirement h5py<2.11.0,>=2.10.0, but you'll have h5py 2.9.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tf-nightly-gpu 2.2.0.dev20200121 has requirement scipy==1.4.1; python_version >= \"3\", but you'll have scipy 1.3.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: tensorboard, tensorflow-estimator, astor, gast, tensorflow\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.3.2\n",
      "    Uninstalling gast-0.3.2:\n",
      "      Successfully uninstalled gast-0.3.2\n",
      "Successfully installed astor-0.8.1 gast-0.2.2 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==1.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "git is already the newest version (1:2.17.1-1ubuntu0.5).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 9 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!apt-get --yes install git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/deepmipt/bert.git@feat/multi_gpu\n",
      "  Cloning https://github.com/deepmipt/bert.git (to revision feat/multi_gpu) to /tmp/pip-req-build-n5z52by4\n",
      "  Running command git clone -q https://github.com/deepmipt/bert.git /tmp/pip-req-build-n5z52by4\n",
      "  Running command git checkout -b feat/multi_gpu --track origin/feat/multi_gpu\n",
      "  Switched to a new branch 'feat/multi_gpu'\n",
      "  Branch 'feat/multi_gpu' set up to track remote branch 'feat/multi_gpu' from 'origin'.\n",
      "Building wheels for collected packages: bert-dp\n",
      "  Building wheel for bert-dp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bert-dp: filename=bert_dp-1.0-py3-none-any.whl size=20137 sha256=a43e7cfe1f22037b776e18e581bcd4e1e00e9e1c8a50b77827aa66e0ce2ae057\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-n9p_zw2k/wheels/65/8c/cb/bb585d79f235c48068b6f4f70f99118fedc3d964d16b1e2caa\n",
      "Successfully built bert-dp\n",
      "Installing collected packages: bert-dp\n",
      "Successfully installed bert-dp-1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/deepmipt/bert.git@feat/multi_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/pristavki.csv', header=None, names=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-21 20:38:41.148 INFO in 'deeppavlov.download'['download'] at line 117: Skipped http://files.deeppavlov.ai/deeppavlov_data/bert/multi_cased_L-12_H-768_A-12.zip download because of matching hashes\n",
      "2020-01-21 20:38:41.153 WARNING in 'deeppavlov.dataset_readers.conll2003_reader'['conll2003_reader'] at line 96: Skip ' \\n', splitted as []\n",
      "2020-01-21 20:38:41.153 WARNING in 'deeppavlov.dataset_readers.conll2003_reader'['conll2003_reader'] at line 96: Skip '.O\\n', splitted as ['.O']\n",
      "2020-01-21 20:38:41.156 INFO in 'deeppavlov.core.trainers.fit_trainer'['fit_trainer'] at line 68: NNTrainer got additional init parameters ['pytest_max_batches', 'pytest_batch_size'] that will be ignored:\n",
      "2020-01-21 20:38:41.735 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /root/data/tag.dict]\n",
      "2020-01-21 20:38:41.737 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 101: [saving vocabulary to /root/data/tag.dict]\n",
      "2020-01-21 20:39:09.910 INFO in 'deeppavlov.models.bert.bert_sequence_tagger'['bert_sequence_tagger'] at line 251: [initializing model with Bert from /root/.deeppavlov/downloads/bert_models/multi_cased_L-12_H-768_A-12/bert_model.ckpt]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /root/.deeppavlov/downloads/bert_models/multi_cased_L-12_H-768_A-12/bert_model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-21 20:39:14.769 DEBUG in 'deeppavlov.metrics.fmeasure'['fmeasure'] at line 394: processed 50 tokens with 2 phrases; found: 2 phrases; correct: 0.\n",
      "\n",
      "precision:  0.00%; recall:  0.00%; FB1:  0.00\n",
      "\n",
      "\tGAME: precision:  0.00%; recall:  0.00%; F1:  0.00 2\n",
      "\n",
      "\n",
      "2020-01-21 20:39:14.770 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 198: Initial best ner_f1 of 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4, \"metrics\": {\"ner_f1\": 0, \"ner_token_f1\": 28.0702}, \"time_spent\": \"0:00:02\", \"epochs_done\": 0, \"batches_seen\": 0, \"train_examples_seen\": 0, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-21 20:40:36.154 DEBUG in 'deeppavlov.metrics.fmeasure'['fmeasure'] at line 394: processed 151 tokens with 17 phrases; found: 17 phrases; correct: 0.\n",
      "\n",
      "precision:  100.00%; recall:  100.00%; FB1:  100.00\n",
      "\n",
      "\tGAME: precision:  100.00%; recall:  100.00%; F1:  100.00 17\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deeppavlov/core/trainers/nn_trainer.py:249: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-21 20:40:36.336 DEBUG in 'deeppavlov.metrics.fmeasure'['fmeasure'] at line 394: processed 50 tokens with 2 phrases; found: 2 phrases; correct: 0.\n",
      "\n",
      "precision:  100.00%; recall:  100.00%; FB1:  100.00\n",
      "\n",
      "\tGAME: precision:  100.00%; recall:  100.00%; F1:  100.00 2\n",
      "\n",
      "\n",
      "2020-01-21 20:40:36.339 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 206: Improved best ner_f1 of 100.0\n",
      "2020-01-21 20:40:36.339 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 208: Saving model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"eval_examples_count\": 4, \"metrics\": {\"ner_f1\": 100.0, \"ner_token_f1\": 100.0}, \"time_spent\": \"0:01:24\", \"epochs_done\": 19, \"batches_seen\": 20, \"train_examples_seen\": 80, \"head_learning_rate\": 0.009999999776482582, \"bert_learning_rate\": 1.9999999552965164e-05, \"loss\": 23.49057698249817}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-21 20:40:36.434 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 75: [saving model to /root/data/model]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4, \"metrics\": {\"ner_f1\": 100.0, \"ner_token_f1\": 100.0}, \"time_spent\": \"0:01:24\", \"epochs_done\": 19, \"batches_seen\": 20, \"train_examples_seen\": 80, \"impatience\": 0, \"patience_limit\": 100}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-21 20:41:20.889 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /root/data/tag.dict]\n",
      "2020-01-21 20:41:47.188 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /root/data/model]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /root/data/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-21 20:41:52.489 DEBUG in 'deeppavlov.metrics.fmeasure'['fmeasure'] at line 394: processed 50 tokens with 2 phrases; found: 2 phrases; correct: 0.\n",
      "\n",
      "precision:  100.00%; recall:  100.00%; FB1:  100.00\n",
      "\n",
      "\tGAME: precision:  100.00%; recall:  100.00%; F1:  100.00 2\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 4, \"metrics\": {\"ner_f1\": 100.0, \"ner_token_f1\": 100.0}, \"time_spent\": \"0:00:02\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-21 20:41:52.763 DEBUG in 'deeppavlov.metrics.fmeasure'['fmeasure'] at line 394: processed 73 tokens with 15 phrases; found: 15 phrases; correct: 0.\n",
      "\n",
      "precision:  93.33%; recall:  93.33%; FB1:  93.33\n",
      "\n",
      "\tGAME: precision:  93.33%; recall:  93.33%; F1:  93.33 15\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"test\": {\"eval_examples_count\": 2, \"metrics\": {\"ner_f1\": 93.3333, \"ner_token_f1\": 96.8421}, \"time_spent\": \"0:00:01\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-21 20:41:53.744 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /root/data/tag.dict]\n",
      "2020-01-21 20:42:19.767 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /root/data/model]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /root/data/model\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from deeppavlov import configs, build_model, train_model\n",
    "\n",
    "with configs.ner.ner_ontonotes_bert_mult.open(encoding='utf8') as f:\n",
    "    ner_config = json.load(f)\n",
    "\n",
    "ner_config['dataset_reader']['data_path'] = 'data'  # directory with train.txt, valid.txt and test.txt files\n",
    "ner_config['metadata']['variables']['NER_PATH'] = 'data'\n",
    "ner_config['metadata']['download'] = [ner_config['metadata']['download'][-1]]  # do not download the pretrained ontonotes model\n",
    "\n",
    "ner_model = train_model(ner_config, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['мортал', 'комбат'],\n",
       "  ['продам', 'фифу'],\n",
       "  ['в', 'комплекте', 'идет', 'Need', 'for', 'speed'],\n",
       "  ['Need', 'far', 'speed'],\n",
       "  ['Neet', 'for', 'speed']],\n",
       " [['O', 'O'],\n",
       "  ['O', 'O'],\n",
       "  ['O', 'O', 'O', 'B-GAME', 'I-GAME', 'I-GAME'],\n",
       "  ['O', 'I-GAME', 'I-GAME'],\n",
       "  ['O', 'O', 'O']]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_model(['мортал комбат', 'продам фифу', 'в комплекте идет Need for speed', 'Need far speed', \"Neet for speed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель чувствительна к регистру и к раскладке, при создании обучающей выборки стоит добавлять примеры как на кириллице, так и на латинице, учитывать возможные склейки слов, сокращения, слэнг. В целом эти же утверждения справедливы и для yargy, и можно сказать, что и в том и в другом случае наша задача состоит в том, чтобы максимально предугадать возможные способы обозначения именованных сущностей и создать что-то вроде словаря. Однако у Берта все-таки есть примущество, заключающееся в том, что он не так чувствителен к разных формам словоизменительной парадигмы (для yargy приходится прописывать всю парадигму в правилах, иначе какие-то формы просто не будут учитываться, Берт же понимает, что это не разные слова). \n",
    "Интересен вопрос с опечатками, в yargy все опечатки должны быть прописаны явно, иначе он просто не учтет такие слова, с Бертом все немного сложнее, он ломается, если опечатка есть в первом слове именнованной сущности, но даже если и не в первом, то отрабатывает некорректно, соответсвенно, для лучше работы надо бы добавить и варианты с опечатками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
